# load pre-trained weights of the GPT-2 model in place of pretraining text
# Pre-training on unlabeled data might require significant amount of compute (atleast A100 GPUs). In such cases we adopt to the pre-trained weights of the GPT-2
# available open-source from the Open-AI repo.

